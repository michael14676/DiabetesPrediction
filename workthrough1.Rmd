---
title: "project"
author: "Michael De La Rosa"
date: "2024-11-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Getting dataset

```{r}
diabetes <- read.csv('diabetes.csv')
head(diabetes)
summary(diabetes)
```

```{r}
library(ggplot2)

par(mfrow=c(3,3))
# Loop through each column
for (col in names(diabetes)) {
  # Check if the column is numeric
  hist(diabetes[[col]],
       main=col,
       xlab=col)
}


```




Notable things:
- Glucose, BloodPressure, SkinThickness, BMI, and Insulin all have values of 0 or near zero that are not real. must figure out a way to deal with these.
- there is no missing data. it does appear that the missing data has been imputed; the zeros were imputed in, whether intentionally or not.
- essentially only BP and BMI are normal (once you take out the weird fake values). you need to do box cox or yeo johnson transformations on everything





# Preprocessing

All of them are continuous except the outcome variable.

For preprocessing, things to do include:
- check for multicolinearity
- check for correlation
- deal with missing/unrealistic ones: we will convert these into NaN and then impute them/remove them.
- near zero var
- check for outliers and normality problems (i expect fixing zeros will help in this)
- center and scaling

## Correlation

```{r correlation}
library(dplyr)
library(corrplot)

corrs <- cor(diabetes[,-9])
corrplot(corrs)

```


If we look, the highest correlation is age and pregnancies which makes sense. The correlation is at 0.5, so we do not need to remove it. Now for multicolinearity.

## Multicolinearity

```{r multicolinearity}
library(car)

# Fit a logistic regression model using GLM
glm_model <- glm(Outcome ~ ., data = diabetes, family = binomial)
# Calculate VIF for the GLM model
vif_values <- vif(glm_model)

# Print VIF values
print(vif_values)

```

None of the VIF are higher than 10, no issues with multicolinearity.


## Missing

Glucose, BloodPressure, SkinThickness, BMI all have values of 0 which are impossible. They are not outliers, but rather incorrectly inputted information. These should be treated as missing values. Let us change them to missing values and then repeat MICE, to see what we should do for imputation.

```{r}
incorrect.cols <- c('Glucose','BloodPressure','SkinThickness','Insulin','BMI') # has the messed up columns
diabetes.corrected <- diabetes


# Loop through each column in the vector
for (col in incorrect.cols) {
  # Replace 0 with NaN
  diabetes.corrected[[col]][diabetes.corrected[[col]] == 0] <- NaN
}

# Print the updated dataframe to verify changes
summary(diabetes.corrected)

```

Now using MICE:

```{r missing.data}
library(VIM)
library(mice)

# Summary of the missing data pattern
md_pattern <- md.pattern(diabetes.corrected)
print(md_pattern)

aggr_plot <- aggr(diabetes.corrected, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(diabetes.corrected), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))


```


Let's look at a few of these that are missing and see if there is something about them: 

```{r}
# Check if NaN values exist (in R, NaN can be checked with is.nan)
rows_with_nan <- is.nan(diabetes.corrected$SkinThickness)

# Create a table of the 'Outcome' column for those rows
outcome_table <- table(diabetes.corrected$Outcome[rows_with_nan])

# Print the table
print(outcome_table)

```




### Imputation of missing values

We will use MICE to generate 5 imputed datasets. This can help deal with variation of the missing data. For now we will only use the first one.

```{r impute}
diabetes.imputed <- mice(diabetes.corrected, m=5, method='pmm') # generates 5 imputed datasets
# summary(diabetes.imputed)

diabetes.preprocessed <- complete(diabetes.imputed, 1) # returns first imputed dataset



```


Now let's look at the histograms again:

```{r}
par(mfrow=c(3,3))
# Loop through each column
for (col in names(diabetes.preprocessed)) {
  # Check if the column is numeric
  hist(diabetes.preprocessed[[col]],
       main=col,
       xlab=col)
}


pairs(diabetes.preprocessed)
```

Note, we also see some new correlation between BMI and skin thickness. I checked and it is 0.63 ish. High but not an issue. There may be randomness with MICE so this may be something to revisit later on, but I don't think it will be an issue.

## Degenerate distributions


```{r degen}
library(caret)

degenerate <- nearZeroVar(diabetes.preprocessed)
```

There are no degenerate variables. We can move on.

## Outliers and normality problems


```{r outliers}

model <- glm(Outcome ~ ., data = diabetes.preprocessed, family = binomial)

# Calculate Cook's Distance
cooks <- cooks.distance(model)

# Plot Cook's Distance
plot(cooks, type = "h", col = "blue", lwd = 2, ylab = "Cook's Distance", main = "Cook's Distance")

# Add a horizontal line for the threshold (typically 4/(n-k-1))
# abline(h = 4/(nrow(diabetes.preprocessed) - length(model$coefficients)), col = "red", lty = 2)
abline(h = 4/(nrow(diabetes.preprocessed)), col = "red", lty = 2)


# Identify potentially influential points
# influential_points <- cooks[cooks > 4/(nrow(diabetes.preprocessed) - length(model$coefficients))]
influential_points <- cooks[cooks > 4/(nrow(diabetes.preprocessed))]

length(influential_points)

```

50 influential points, about 1/16 of the data is influential points. That is significant. Let's see what kind of influential points these are:

```{r whats.influential}
influential.indices <- which(cooks > 4/(nrow(diabetes.preprocessed)))
diabetes.influential <- diabetes.preprocessed[influential.indices,]
summary(diabetes.influential)


```

For right now, they seem problematic. Let's remove them.

```{r}
diabetes.no.influential <- diabetes.preprocessed[-influential.indices,]
#diabetes.no.influential <- diabetes.preprocessed

# recalculate cooks distance
model1 <- glm(Outcome ~ ., data = diabetes.no.influential, family = binomial)

# Calculate Cook's Distance
cooks1 <- cooks.distance(model1)

# Plot Cook's Distance
plot(cooks1, type = "h", col = "blue", lwd = 2, ylab = "Cook's Distance", main = "Cook's Distance")

# Add a horizontal line for the threshold (typically 4/(n-k-1))
# abline(h = 4/(nrow(diabetes.preprocessed) - length(model$coefficients)), col = "red", lty = 2)
abline(h = 4/(nrow(diabetes.no.influential)), col = "red", lty = 2)

influential_points <- cooks[cooks > 4/(nrow(diabetes.no.influential))]
length(influential_points)
```

Some outliers are still present, but the magnitude is much lower. We can leave them in for now and just keep it in the back of our minds. 


## Centering and scaling

```{r center.scale}
diabetes.complete <- as.data.frame(scale(diabetes.no.influential[,-9]))
diabetes.complete$Outcome <- as.factor(diabetes.no.influential$Outcome)


par(mfrow=c(3,3))
# Loop through each column
for (col in names(diabetes.complete[,-9])) {
  # Check if the column is numeric
  hist(diabetes.complete[[col]],
       main=col,
       xlab=col)
}
```

Things are looking good. We can move on to predictions and see what we need to fix.

# Predictions

We can do 10 fold validation using logistic regression to start with. The data is slightly imbalanced but it is around 2 negatives for 1 positive, so we don't need to worry too much about SMOTE or any other types of oversampling for the positive class.

## Creating folds

```{r k.fold}
library(MASS)
library(pROC)

k = 10

fold.indices <- createDataPartition(diabetes.complete$Outcome, times = k, p = 1/k, list = TRUE) # list of 10 lists of indices for testing
```

Sanity check on properly balanced data:

```{r k.fold.balance}
par(mfrow = c(2,5))
for (fold in fold.indices){
  print('test')
  barplot(table(diabetes[fold,9]))
  print('train')
  barplot(table(diabetes[-fold,9]))
}
```

The balancing looks fine. We can continue to training.

## Training

```{r lr}
stepwise.logistic <- function(test.index) {
  train <- diabetes.complete[-test.index,]
  test <- diabetes.complete[test.index,]
  full.model <- glm(Outcome ~ ., data=train, family = binomial)
  null.model <- glm(Outcome ~ 1, data=train, family = binomial)
  
  stepwise <- stepAIC(null.model, scope = list(lower = null.model, upper = full.model), direction = 'both', trace=0)
  
  y.prob <- predict(stepwise, newdata = test, type='response')
  
  y.pred <- ifelse(y.prob > 0.5, '1', '0')
  
  # print(y.pred)
  y.pred <- factor(y.pred, levels = c(0, 1))
  test$Outcome <- factor(test$Outcome, levels = c(0, 1))
  
  cm <- confusionMatrix(y.pred, reference = test$Outcome, positive = '1', mode='everything')
  roc.obj <- roc(test$Outcome, y.prob)
  
  return(list(cm = cm, roc.obj = roc.obj, predictors = stepwise$formula))
}

metrics <- lapply(fold.indices, stepwise.logistic)

```


## Metrics

Going through the confusion matrix and AUROC:

```{r}
auc.list <- list()
# Print confusion matrix and auroc
for (i in seq_along(metrics)) {
  cat("=========== Fold", i, "===========\n")
  x <- metrics[[i]]
  cm <- x[[1]]
  roc.obj <- x[[2]]
  formula <- x[[3]]
  print(cm)
  auc <- auc(roc.obj)
  print(auc)
  auc.list <- append(auc.list, as.numeric(auc))
  print(formula)
  cat("\n")
}
```

The performance is good, with a range in the 0.8 region. Here is the average AUROC:


```{r}
plot(roc.obj)
print(mean(unlist(auc.list)))
```

The average AUC is 0.86.

## KNN

Next step would be to do other models. Probably pretty similar to LDA so I don't think we would really need to do that. Perhaps QDA? This is probably way too small for NB so we can probably skip that. I would also say a KNN classifier just to see, and then either/both ridge and lasso. There is no multicolinearity issues here so lasso would probably be a good choice.

We would also need nice plots, so we can mess around with ggplot2 to compare whatever models we choose in the end.
```{r}
# Splitting the dataset into predictors and target variable
set.seed(123)  # For reproducibility
x <- diabetes.preprocessed[, -which(names(diabetes.preprocessed) == "Outcome")]
y <- diabetes.preprocessed$Outcome

# Standardize predictors
x.scaled <- scale(x)

# Define trainControl for k-fold cross-validation
ctrl <- trainControl(method = "cv", number = 10)  # 10-fold CV

# Perform kNN with cross-validation to tune k
set.seed(123)
knn.tuned <- train(
  x = x.scaled,
  y = as.factor(y),  # Ensure y is treated as a classification problem
  method = "knn",
  tuneGrid = data.frame(k = seq(1, 20, by = 2)),  # Grid of k values to test
  trControl = ctrl
)

# Print the results
print(knn.tuned)
plot(knn.tuned)

# Best k and corresponding accuracy
best.k <- knn.tuned$bestTune$k
best.accuracy <- max(knn.tuned$results$Accuracy)

cat("Best k:", best.k, "\n")
cat("Best Accuracy:", best.accuracy, "\n")
```

```{r}
# Splitting the dataset into training and test sets
set.seed(123)  # For reproducibility
tr.ind <- sample(seq_len(nrow(diabetes.complete)), floor(0.7 * nrow(diabetes.complete)), replace = FALSE)
x.tr <- diabetes.complete[tr.ind, -which(names(diabetes.complete) == "Outcome")]
x.te <- diabetes.complete[-tr.ind, -which(names(diabetes.complete) == "Outcome")]
y.tr <- diabetes.complete$Outcome[tr.ind]
y.te <- diabetes.complete$Outcome[-tr.ind]

# Logistic Regression (Full Model)
model.logistic <- glm(Outcome ~ ., data = diabetes.complete[tr.ind,], family = binomial)
pred.logistic <- predict(model.logistic, newdata = diabetes.complete[-tr.ind,], type = "response")
class.logistic <- ifelse(pred.logistic > 0.5, 1, 0)
accuracy.logistic <- mean(class.logistic == y.te)

# Lasso Regression
library(glmnet)
lasso.cv <- cv.glmnet(as.matrix(x.tr), y.tr, family = "binomial", alpha = 1)
lasso.fit <- glmnet(as.matrix(x.tr), y.tr, family = "binomial", alpha = 1)
lasso.predict <- predict(lasso.fit, s = lasso.cv$lambda.1se, newx = as.matrix(x.te), type = "response")
class.lasso <- ifelse(lasso.predict > 0.5, 1, 0)
accuracy.lasso <- mean(class.lasso == y.te)

# Ridge Regression
ridge.cv <- cv.glmnet(as.matrix(x.tr), y.tr, family = "binomial", alpha = 0)
ridge.fit <- glmnet(as.matrix(x.tr), y.tr, family = "binomial", alpha = 0)
ridge.predict <- predict(ridge.fit, s = ridge.cv$lambda.1se, newx = as.matrix(x.te), type = "response")
class.ridge <- ifelse(ridge.predict > 0.5, 1, 0)
accuracy.ridge <- mean(class.ridge == y.te)

# Elastic Net Regression
enet.cv <- cv.glmnet(as.matrix(x.tr), y.tr, family = "binomial", alpha = 0.5)
enet.fit <- glmnet(as.matrix(x.tr), y.tr, family = "binomial", alpha = 0.5)
enet.predict <- predict(enet.fit, s = enet.cv$lambda.1se, newx = as.matrix(x.te), type = "response")
class.enet <- ifelse(enet.predict > 0.5, 1, 0)
accuracy.enet <- mean(class.enet == y.te)

# Quadratic Discriminant Analysis (QDA)
library(MASS)
qda.model <- qda(x = x.tr, grouping = y.tr)
qda.pred <- predict(qda.model, newdata = x.te)
class.qda <- qda.pred$class  # Extract predicted classes
accuracy.qda <- mean(class.qda == y.te)

# k-Nearest Neighbors (kNN)
library(class)
# Standardize predictors for kNN
x.tr.scaled <- scale(x.tr)
x.te.scaled <- scale(x.te, center = attr(x.tr.scaled, "scaled:center"), scale = attr(x.tr.scaled, "scaled:scale"))

# Best k = 15
k <- 15
knn.pred <- knn(train = x.tr.scaled, test = x.te.scaled, cl = y.tr, k = k)
accuracy.knn <- mean(knn.pred == y.te)

# Combine Results into a Data Frame
accuracy_comparison <- data.frame(
  Model = c("Logistic Regression", "Lasso", "Ridge", "Elastic Net", "QDA", "kNN"),
  Accuracy = c(accuracy.logistic, accuracy.lasso, accuracy.ridge, accuracy.enet, accuracy.qda, accuracy.knn)
)

print(accuracy_comparison)
```


































































```{r}
# Scale the entire dataset
scaled_data <- scale(diabetes.complete[, -which(names(diabetes.complete) == "Outcome")])
target <- as.factor(diabetes.complete$Outcome)  # Ensure target is a factor for classification

# Cross-validation to find the best k
train_control <- trainControl(method = "cv", number = 10)  # 10-fold CV for tuning
knn.tuned <- train(
    x = scaled_data,
    y = target,
    method = "knn",
    trControl = train_control,
    tuneGrid = data.frame(k = seq(1, 100, by = 2))  # Test odd k values from 1 to 100
)

# Extract the best k
best.k <- knn.tuned$bestTune$k
cat("Best k determined from CV:", best.k, "\n")
```

### Training and Evaluation


```{r}
# Load required libraries
library(caret)
library(pROC)
library(randomForest)
library(glmnet)
library(class)
library(MASS)
library(pls)

# Initialize lists to store metrics for each model
cm.list <- list()
roc.list <- list()
auc.list <- list()

# Splitting the dataset
set.seed(123)  # For reproducibility
fold.indices <- createFolds(diabetes.complete$Outcome, k = 10, list = TRUE)

# Function to train and evaluate models
evaluate_model <- function(model_name, train, test, y.train, y.test) {
  if (model_name == "stepwise logistic") {
    logistic.full <- glm(Outcome ~ ., data=data.frame(train, Outcome=y.train), family=binomial)
    logistic.null <- glm(Outcome ~ 1, data=data.frame(train, Outcome=y.train), family=binomial)
    logistic.model <- stepAIC(logistic.null, scope=list(lower=logistic.null, upper=logistic.full), direction='both', trace=0)
    y.prob <- predict(logistic.model, newdata=data.frame(test), type="response")
  } else if (model_name == "full logistic") {
    logistic.full <- glm(Outcome ~ ., data=data.frame(train, Outcome=y.train), family=binomial)
    y.prob <- predict(logistic.full, newdata=data.frame(test),  type="response")
  } else if (model_name == "lasso") {
    lasso.cv <- cv.glmnet(as.matrix(train), y.train, family = "binomial", alpha = 1)
    lasso.model <- glmnet(as.matrix(train), y.train, family = "binomial", alpha = 1)
    y.prob <- predict(lasso.model, s = lasso.cv$lambda.1se, newx = as.matrix(test), type = "response")
  } else if (model_name == "ridge") {
    ridge.cv <- cv.glmnet(as.matrix(train), y.train, family = "binomial", alpha = 0)
    ridge.model <- glmnet(as.matrix(train), y.train, family = "binomial", alpha = 0)
    y.prob <- predict(ridge.model, s = ridge.cv$lambda.1se, newx = as.matrix(test), type = "response")
  } else if (model_name == "enet") {
    enet.cv <- cv.glmnet(as.matrix(train), y.train, family = "binomial", alpha = 0.5)
    enet.model <- glmnet(as.matrix(train), y.train, family = "binomial", alpha = 0.5)
    y.prob <- predict(enet.model, s = enet.cv$lambda.1se, newx = as.matrix(test), type = "response")
  } else if (model_name == "qda") {
    qda.model <- qda(x = train, grouping = y.train)
    y.prob <- predict(qda.model, newdata = test)$posterior[, 2]
  } else if (model_name == "knn") {
    # Scale training and test data
    train.scaled <- scale(train)
    test.scaled <- scale(test, center = attr(train.scaled, "scaled:center"), scale = attr(train.scaled, "scaled:scale"))
    
    # Train final kNN model using best k
    best.k <- 25
    y.prob <- as.numeric(knn(train = train.scaled, test = test.scaled, cl = y.train, k = best.k)) - 1
  } else if (model_name == "rf") {
    rf.model <- randomForest(as.factor(y.train) ~ ., data = data.frame(train, y.train = y.train))
    y.prob <- predict(rf.model, newdata = data.frame(test), type = "prob")[, 2]
  } else if (model_name == "pcr") {
    # Convert predictors and response to numeric
    train <- data.matrix(train)
    test <- data.matrix(test)
    y.train <- as.numeric(as.character(y.train))
    y.test <- as.numeric(as.character(y.test))
    
    # Fit PCR model with cross-validation
    pcr.model <- pcr(y.train ~ train, validation = "CV")
    
    # Determine the optimal number of components, ensuring it's valid
    max_comps <- dim(pcr.model$scores)[2]
    ncomp <- min(which.min(RMSEP(pcr.model)$val[1, , ]), max_comps)
    
    # Generate predictions using the selected number of components
    y.prob <- predict(pcr.model, newdata = test, ncomp = ncomp)
  } else if (model_name == "pls") {
    # Convert predictors and response to numeric
    train <- data.matrix(train)
    test <- data.matrix(test)
    y.train <- as.numeric(as.character(y.train))
    y.test <- as.numeric(as.character(y.test))
    
    # Fit PLS model with cross-validation
    pls.model <- plsr(y.train ~ train, validation = "CV")
    
    # Determine the optimal number of components, ensuring it's valid
    max_comps <- dim(pls.model$scores)[2]
    ncomp <- min(which.min(RMSEP(pls.model)$val[1, , ]), max_comps)
    
    # Generate predictions using the selected number of components
    y.prob <- predict(pls.model, newdata = test, ncomp = ncomp)
  }

  y.pred <- ifelse(y.prob > 0.5, '1', '0')
  y.pred <- factor(y.pred, levels=c(0,1))
  y.test <- factor(y.test, levels=c(0,1))
  cm <- confusionMatrix(y.pred, reference=y.test, positive='1', mode='everything')
  roc.obj <- roc(y.test, y.prob)
  auc.val <- auc(roc.obj)
  return(list(cm = cm, roc.obj = roc.obj, auc.val = auc.val))
}

# Loop through models and folds
# models <- c("logistic", "lasso", "ridge", "enet", "qda", "knn", "rf", "pcr", "pls")

models <- c('stepwise logistic', 'full logistic', 'qda', 'knn', 'rf')

for (model_name in models) {
  cat("Evaluating:", model_name, "\n")
  model.cm.list <- list()
  model.roc.list <- list()
  model.auc.list <- c()
  
  for (i in seq_along(fold.indices)) {
    cat("Fold", i, "\n")
    train.indices <- fold.indices[[i]]
    train <- diabetes.complete[train.indices, -which(names(diabetes.complete) == "Outcome")]
    test <- diabetes.complete[-train.indices, -which(names(diabetes.complete) == "Outcome")]
    y.train <- diabetes.complete$Outcome[train.indices]
    y.test <- diabetes.complete$Outcome[-train.indices]
    
    metrics <- evaluate_model(model_name, train, test, y.train, y.test)
    model.cm.list[[i]] <- metrics$cm
    model.roc.list[[i]] <- metrics$roc.obj
    model.auc.list <- c(model.auc.list, metrics$auc.val)
  }
  cm.list[[model_name]] <- model.cm.list
  roc.list[[model_name]] <- model.roc.list
  auc.list[[model_name]] <- mean(model.auc.list)
  cat("Mean AUC for", model_name, ":", mean(model.auc.list), "\n\n")
}

# Print overall results
print(auc.list)
#print(cm.list)

```
### Plotting ROC curves

```{r}
# Plot ROC curves for a specific model (e.g., "logistic")
model_name <- "knn"
for (i in seq_along(roc.list[[model_name]])) {
  plot(roc.list[[model_name]][[i]], col = i, add = i > 1, main = paste("ROC for", model_name))
}
```

